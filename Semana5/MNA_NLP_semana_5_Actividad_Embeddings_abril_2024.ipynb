{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Adtividad Semana 5\n",
        "\n",
        "### **Vectores Embebidos Pre-entrenados: Fasttext**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "* José Alberto Mtanous Treviño\n",
        "* Javier Muñoz Barrios\n",
        "* José Francisco Muñoz Del Ángel\n",
        "* César Alexis Nájera Mendoza\n",
        "* Annette Cristina Narváez Andrade\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wCL2p6MA8NuT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/jmtanous/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/jmtanous/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/jmtanous/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('punkt')    # es un tokenizador que ayuda a dividr el texto en enunciados mediante un modelo no-supervisado.\n",
        "nltk.download('stopwords')    # para tener acceso a \"stopwords\" en varios idiomas.\n",
        "\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "##**Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
        "\n",
        "**Llama simplemente \"df\" a dicho DataFrame.**\n",
        "\n",
        "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_lyEFRkxzC6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "df_amazon = pd.read_csv(\"./data/amazon5.txt\", sep='\\t', names= ['review','tag'])\n",
        "df_imdb = pd.read_csv(\"./data/imdb5.txt\", sep=r'\\s{3}', names= ['review','tag'], engine='python')\n",
        "df_yelp = pd.read_csv(\"./data/yelp5.txt\", sep='\\t', names= ['review','tag'])\n",
        "\n",
        "df = pd.concat([df_amazon, df_imdb, df_yelp])\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3-w1xMLYnm9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   tag     3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 70.3+ KB\n"
          ]
        }
      ],
      "source": [
        "# Verifiquemos la información del DataFrame:\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NfVUcYe1nubT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  tag\n",
              "0  So there is no way for me to plug it in here i...    0\n",
              "1                        Good case, Excellent value.    1\n",
              "2                             Great for the jawbone.    1\n",
              "3  Tied to charger for conversations lasting more...    0\n",
              "4                                  The mic is great.    1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Y veamos sus primeros registros:\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZZ0stLmWJN"
      },
      "source": [
        "##**Pregunta - 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6JF5BommZ6"
      },
      "source": [
        "Realiza el proceso de limpieza.\n",
        "\n",
        "Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TsnvMp-7oYCM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>tag</th>\n",
              "      <th>lemmatized_tokens</th>\n",
              "      <th>lemmatized_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "      <td>[no, way, I, plug, us, unless, I, go, converter]</td>\n",
              "      <td>no way I plug us unless I go converter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "      <td>[good, case, excellent, value]</td>\n",
              "      <td>good case excellent value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "      <td>[great, jawbone]</td>\n",
              "      <td>great jawbone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "      <td>[tie, charger, conversation, last, 45, minute,...</td>\n",
              "      <td>tie charger conversation last 45 minute major ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "      <td>[mic, great]</td>\n",
              "      <td>mic great</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  tag  \\\n",
              "0  So there is no way for me to plug it in here i...    0   \n",
              "1                        Good case, Excellent value.    1   \n",
              "2                             Great for the jawbone.    1   \n",
              "3  Tied to charger for conversations lasting more...    0   \n",
              "4                                  The mic is great.    1   \n",
              "\n",
              "                                   lemmatized_tokens  \\\n",
              "0   [no, way, I, plug, us, unless, I, go, converter]   \n",
              "1                     [good, case, excellent, value]   \n",
              "2                                   [great, jawbone]   \n",
              "3  [tie, charger, conversation, last, 45, minute,...   \n",
              "4                                       [mic, great]   \n",
              "\n",
              "                                  lemmatized_reviews  \n",
              "0             no way I plug us unless I go converter  \n",
              "1                          good case excellent value  \n",
              "2                                      great jawbone  \n",
              "3  tie charger conversation last 45 minute major ...  \n",
              "4                                          mic great  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "mystopwords = [stop_word for stop_word in stopwords.words('english') if stop_word not in negwords]\n",
        "\n",
        "\n",
        "lemmatized_tokens = []\n",
        "lemmatized_reviews = []\n",
        "for review in df['review']:\n",
        "    review = re.sub(r'\\.',r' ', review)\n",
        "    review = review.lower()\n",
        "    review = re.sub(r'\\s+',r' ', review)\n",
        "    doc = nlp(review)\n",
        "    _lemmatized_tokens = [token.lemma_ for token in doc if token.lemma_ not in mystopwords]\n",
        "    _lemmatized_tokens = [token for token in _lemmatized_tokens if not re.match(r'\\W', token)]\n",
        "\n",
        "    lemmatized_tokens.append(_lemmatized_tokens)\n",
        "    lemmatized_reviews.append(' '.join(_lemmatized_tokens))\n",
        "\n",
        "df['lemmatized_tokens'] = lemmatized_tokens\n",
        "df['lemmatized_reviews'] = lemmatized_reviews\n",
        "df.head()\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7jlQuoI2o33T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['no', 'way', 'I', 'plug', 'us', 'unless', 'I', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tie', 'charger', 'conversation', 'last', '45', 'minute', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "# Despleguemos los primeros comentarios después de tu proceso de limpieza:\n",
        "\n",
        "Xclean = df['lemmatized_tokens']\n",
        "\n",
        "for x in Xclean[0:5]:\n",
        "  print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygchEdcKqIzU"
      },
      "source": [
        "#**Pregunta - 3:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEIOkkl9Dot"
      },
      "source": [
        "\n",
        "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b0SAcYdq9X0w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ************* Inicia la sección de agregar código:*****************************\n",
        "from sklearn.model_selection import train_test_split\n",
        "Y = df['tag']\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1) \n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "# *********** Termina la sección de agregar código *************\n",
        "\n",
        "\n",
        "# verificemos las dimensiones obtenidas:\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjKoEqiqBN1"
      },
      "source": [
        "#**Pregunta - 4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jENsKiN99r3F"
      },
      "source": [
        "\n",
        "\n",
        "Construye tu vocabulario a continuación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TzJntmLPqPqC"
      },
      "outputs": [],
      "source": [
        "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
        "#     con un tamaño que consideres adecuado:\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "midiccionario = Counter()    \n",
        "\n",
        "_x_train = x_train.to_list()\n",
        "\n",
        "for k in range(len(_x_train)):\n",
        "  midiccionario.update(_x_train[k])\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yTDZ0Rr86CUP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del vocabulario generado:\n",
            "3421\n",
            "Nueva longitud del nuevo vocabulario: 1455\n"
          ]
        }
      ],
      "source": [
        "# b.\tIndica el tamaño del vocabulario generado.\n",
        "\n",
        "print('Longitud del vocabulario generado:')\n",
        "\n",
        "\n",
        "# ******* Inicia la sección de agregar código: ***********\n",
        "print(len(midiccionario))\n",
        "\n",
        "min_freq = 2\n",
        "\n",
        "midiccX = {token: freq for token, freq in midiccionario.items() if freq >= min_freq}\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "print('Nueva longitud del nuevo vocabulario:', len(midiccX))\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDa4EhTqrw15"
      },
      "source": [
        "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
        "\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "Para evaluar correctamente el modelo, no debemos de mezclar datos de entrenamiento con los de validación o prueba. Si pretendemos usar el modelo en un caso real\n",
        "no sabriamos de antemano el vocabulario completo.\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7ykjxQI3rpxx"
      },
      "outputs": [],
      "source": [
        "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
        "#     validación y prueba para que todos los comentarios usen solamente las\n",
        "#     palabras de este vocabulario.\n",
        "\n",
        "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
        "\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "train_x = []\n",
        "val_x = []\n",
        "test_x = []\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n",
        "\n",
        "for tokens in x_train.to_list():\n",
        "    seen_tokens = [token for token in tokens if token in midiccX]\n",
        "    train_x.append(seen_tokens)\n",
        "\n",
        "for tokens in x_val.to_list():\n",
        "    seen_tokens = [token for token in tokens if token in midiccX]\n",
        "    val_x.append(seen_tokens)\n",
        "\n",
        "for tokens in x_test.to_list():\n",
        "    seen_tokens = [token for token in tokens if token in midiccX]\n",
        "    test_x.append(seen_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iYF2RGuPtQTC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['battery', 'completely', 'useless', 'I']\n",
            "['service', 'super', 'friendly']\n",
            "['try', 'make', 'call']\n",
            "['I', 'well', 'atmosphere']\n",
            "['definitely', 'turn', 'I', 'I', 'doubt', 'I', 'back', 'unless', 'someone', 'else', 'buy']\n"
          ]
        }
      ],
      "source": [
        "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
        "\n",
        "for ss in train_x[0:5]:\n",
        "  print(ss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0Hxj25vTWh"
      },
      "source": [
        "#**Pregunta - 5:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnHHAza5_P5Z"
      },
      "source": [
        "\n",
        "a. Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTI9xSgF_Xc8"
      },
      "source": [
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "| Modelo        | Pros                                                                 | Contras                                                                |\n",
        "|---------------|----------------------------------------------------------------------|------------------------------------------------------------------------|\n",
        "| **FastText**  | - Considera sub-palabras, lo que mejora la representación de palabras raras y errores ortográficos. <br> - Mejor rendimiento en idiomas morfológicamente ricos. <br> - Rápido y eficiente en términos de memoria. | - Mayor complejidad en comparación con Word2Vec. <br> - Entrenamiento más lento debido a la consideración de sub-palabras. |\n",
        "| **Word2Vec**  | - Simplicidad y rapidez en el entrenamiento. <br> - Amplio uso y soporte en la comunidad. <br> - Dos modos de entrenamiento: Skip-gram y CBOW. | - No maneja bien palabras raras o errores ortográficos. <br> - No captura información de sub-palabras. |\n",
        "| **GloVe**     | - Captura relaciones semánticas de manera más efectiva a través de una matriz de co-ocurrencia global. <br> - Buen rendimiento en tareas de similaridad semántica. | - Entrenamiento más complejo y lento comparado con Word2Vec. <br> - Requiere más memoria debido al uso de matrices grandes. |\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqRl7fT_fn2"
      },
      "source": [
        "#**Pregunta - 6:**\n",
        "\n",
        "Utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300.\n",
        "\n",
        "Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
        "\n",
        "Es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo.\n",
        "\n",
        "Recuerda borrar la variable donde descargaste los 2 millones de vectores embebidos Fasttext.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UdK-jMfLxHLY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "vocabulario = list(midiccX.keys())\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "midicc = {}\n",
        "\n",
        "for token in vocabulario:\n",
        "    midicc.update({token:ft.get_word_vector(token)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4S7q0yR0Mpi"
      },
      "source": [
        "#**Pregunta - 7:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyeOrkoaC1eq"
      },
      "source": [
        "\n",
        "\n",
        "Generamos los vectores embebidos a paertir de los conjuntos de entrenamiento, validación y preuba.\n",
        "\n",
        "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wnfQpkxg0Usq"
      },
      "outputs": [],
      "source": [
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "#for tokens in train_x:\n",
        "#    for token in tokens:\n",
        "trainEmb = []\n",
        "valEmb = []\n",
        "testEmb = []\n",
        "\n",
        "for tokens in train_x:\n",
        "    vectores = []\n",
        "    for token in tokens:\n",
        "        vectores.append(midicc[token])\n",
        "    if len(tokens) == 0:\n",
        "        vectores = np.zeros((1,300))\n",
        "    vectores = np.array(vectores)\n",
        "    trainEmb.append(np.mean(vectores, axis=0))\n",
        "\n",
        "for tokens in val_x:\n",
        "    vectores = []\n",
        "    for token in tokens:\n",
        "        vectores.append(midicc[token])\n",
        "    if len(tokens) == 0:\n",
        "        vectores = np.zeros((1,300))    \n",
        "    vectores = np.array(vectores)\n",
        "    valEmb.append(np.mean(vectores, axis=0))\n",
        "\n",
        "for tokens in test_x:\n",
        "    vectores = []\n",
        "    for token in tokens:\n",
        "        vectores.append(midicc[token])\n",
        "    if len(tokens) == 0:\n",
        "        vectores = np.zeros((1,300))\n",
        "    vectores = np.array(vectores)\n",
        "    testEmb.append(np.mean(vectores, axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J3BBF96D0N8Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Emb: (450, 300)\n",
            "Val-Emb: (450, 300)\n",
            "Test-Emb: (450, 300)\n"
          ]
        }
      ],
      "source": [
        "# Veamos las dimensiones de cada conjunto embebido:\n",
        "trainEmb = np.array(trainEmb)\n",
        "valEmb = np.array(valEmb)\n",
        "testEmb = np.array(testEmb)\n",
        "\n",
        "print(\"Train-Emb:\", trainEmb.shape)\n",
        "print(\"Val-Emb:\", valEmb.shape)\n",
        "print(\"Test-Emb:\", testEmb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibp1LA91CP_"
      },
      "source": [
        "#**Pregunta - 8:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxC9K0VnGOwG"
      },
      "source": [
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
        "\n",
        "Compara los resultados con los de la semana anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ycwjD8ztGOL7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR: Train-accuracy: 85.05%\n",
            "LR: Val-accuracy: 84%\n"
          ]
        }
      ],
      "source": [
        "# REGRESIÓN LOGÍSTICA:\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "\n",
        "\n",
        "modeloLR = LogisticRegression()\n",
        "modeloLR.fit(trainEmb,y_train)\n",
        "\n",
        "\n",
        "\n",
        "# FIN PARA AGREGAR TUS LÍNEAS DE CÓDIGO.\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "print('LR: Train-accuracy: %.2f%%' % (100*modeloLR.score(trainEmb, y_train)))\n",
        "print('LR: Val-accuracy: %2.f%%' % (100*modeloLR.score(valEmb, y_val)))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N4n70GHW0sl3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RF: Train-accuracy: 100.00%\n",
            "RF: Val-accuracy: 80.22%\n"
          ]
        }
      ],
      "source": [
        "# BOSQUE ALEATORIO (Random Forest):\n",
        "\n",
        "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
        "modeloRF = RandomForestClassifier()\n",
        "modeloRF.fit(trainEmb,y_train)\n",
        "\n",
        "print('\\nRF: Train-accuracy: %.2f%%' % (100*modeloRF.score(trainEmb, y_train)))\n",
        "print('RF: Val-accuracy: %.2f%%' % (100*modeloRF.score(valEmb, y_val)))\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDIiSHvg0_hm"
      },
      "source": [
        "#**Pregunta - 9:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJtALGZHrGk"
      },
      "source": [
        "\n",
        "\n",
        "Reporte del mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ETv4VLjP1GYt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test-accuracy con el mejor modelo 85.56%\n",
            "\n",
            "Matriz de confusión con el mejor modelo:\n",
            "[[183  33]\n",
            " [ 32 202]]\n",
            "\n",
            "Matriz de confusión con el mejor modelo en proporciones:\n",
            "[[0.40666667 0.07333333]\n",
            " [0.07111111 0.44888889]]\n"
          ]
        }
      ],
      "source": [
        "# **##############################################################################\n",
        "# AGREGA AQUÍ EL NOMBRE DE TU MEJOR MODELO OBTENIDO CON COUNTER:\n",
        "\n",
        "mejor_modelo = modeloLR # incluye el nombre, modeloXXcount, de tu mejor modelo.\n",
        "\n",
        "# FIN PARA AGREGAR TUS LÍNEAS DE CÓDIGO.\n",
        "##############################################################################\n",
        "\n",
        "print('Test-accuracy con el mejor modelo %.2f%%' % (100*mejor_modelo.score(testEmb, y_test)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = mejor_modelo.predict(testEmb)\n",
        "print('\\nMatriz de confusión con el mejor modelo:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
        "\n",
        "print('\\nMatriz de confusión con el mejor modelo en proporciones:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]) / pred.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkh2WfN1MC1"
      },
      "source": [
        "#**Pregunta - 10:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySFuDQtVuK5"
      },
      "source": [
        "\n",
        "\n",
        "Incluye tus comentarios finales de la actividad.\n",
        "\n",
        "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "### ++++++++ Termina la sección de agregar texto: +++++++++++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKHmQTbWJT1"
      },
      "source": [
        "##**Fin de la Actividad de vectores Embebidos - FastText**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
